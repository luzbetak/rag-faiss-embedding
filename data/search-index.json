[
  {
    "id": 8,
    "url": "https://kevinluzbetak.com/1-ai/Gunning-Fog-Index.html",
    "title": "Gunning-Fog-Index.html",
    "content": "python algorithms gunning fog index the gunning fog index is a readability test that estimates the years of formal education needed to understand a text on the first reading it takes into account the number of words the number of complex words words with three or more syllables and the number of sentences in a text import re def count_words text counts the number of words in a text words re findall r w text return len words def count_sentences text counts the number of sentences in a text sentences re split r text return len sentences 1 if text 1 in else len sentences def count_complex_words text counts the number of complex words in a text words with three or more syllables def syllable_count word word word lower vowels aeiouy count 0 if word 0 in vowels count 1 for index in range 1 len word if word index in vowels and word index 1 not in vowels count 1 if word endswith e count 1 if count 0 count 1 return count words re findall r w text complex_words word for word in words if syllable_count word 3 return len complex_words def gunning_fog_index text calculates the gunning fog index for a given text num_words count_words text num_sentences count_sentences text num_complex_words count_complex_words text if num_sentences 0 return 0 avoid division by zero average_sentence_length num_words num_sentences percentage_complex_words num_complex_words num_words 100 fog_index 0 4 average_sentence_length percentage_complex_words return fog_index example usage text the gunning fog index is a readability test for english writing it estimates the years of formal education needed to understand the text on the first reading complex words are those with three or more syllables fog_index gunning_fog_index text print f the gunning fog index for the given text is fog_index 2f output the gunning fog index for the given text is 10 40 gunning fog index explanation the gunning fog index is a readability test that estimates the years of formal education needed to understand a text on the first reading the implementation calculates the gunning fog index using the following steps words the code counts all words in the text using regular expressions sentences sentences are counted by splitting the text at sentence ending punctuation marks such as period exclamation mark and question mark complex words a complex word is defined as one with three or more syllables the syllable_count function estimates the number of syllables in a word by analyzing vowel patterns gunning fog index calculation the formula used for the gunning fog index is average sentence length number of words number of sentences percentage of complex words number of complex words number of words 100 the gunning fog index is calculated as 0 4 average sentence length percentage of complex words usage a gunning fog index of 7 8 is considered optimal for general readership a higher score suggests that the text is more difficult to read requiring higher levels of education this implementation provides a way to rank texts based on readability using the gunning fog index which is useful in various natural language processing nlp applications such as document analysis content creation and education footer",
    "created_at": "2024-10-31T05:51:10.349483",
    "updated_at": "2024-10-31T05:51:10.349486"
  },
  {
    "id": 4,
    "url": "https://kevinluzbetak.com/1-ai/NVIDIA-GeForce-RTX-3060-Installation.html",
    "title": "NVIDIA-GeForce-RTX-3060-Installation.html",
    "content": "geforce rtx 3060 installation install nvidia geforce rtx 3060 for machine learning step 1 prepare your system before installing the nvidia drivers update your system and install the necessary packages sudo apt update sudo apt upgrade install required build tools and headers sudo apt install build essential linux headers uname r step 2 install nvidia drivers add the nvidia ppa add the official nvidia ppa to get the latest drivers sudo add apt repository ppa graphics drivers ppa sudo apt update identify the recommended driver find the recommended driver for your gpu using the following command ubuntu drivers devices the output will suggest the best driver for your system for example it may recommend nvidia driver 560 install the recommended driver replace nvidia driver xxx with the recommended driver such as nvidia driver 560 sudo apt install nvidia driver 560 reboot your system reboot your machine to apply the changes sudo reboot step 3 install cuda optional if you plan to use cuda for gpu computing you can install it using the following command sudo apt install nvidia cuda toolkit step 4 verify installation after rebooting you can check if the driver is installed and working properly by running alias nv1 nvtop alias nv2 nvidia smi this will display details about your installed gpu and the driver version step 5 enable microphone on chrome insecure origins treated as secure http 192 168 1 190 8080 chrome flags footer",
    "created_at": "2024-10-31T05:51:09.998992",
    "updated_at": "2024-10-31T05:51:09.998995"
  },
  {
    "id": 1,
    "url": "https://kevinluzbetak.com/1-ai/Python-Syntax-Highlighting.html",
    "title": "Python-Syntax-Highlighting.html",
    "content": "kevin luzbetak computer science python syntax highlighting def greet name print f hello name def add a b return a b if __name__ __main__ greet world result add 5 3 print f 5 3 result sql syntax highlighting select name age from users where age 18 order by age desc bash syntax highlighting bin bash echo hello world mkdir new_directory cd new_directory golang syntax highlighting package main import fmt func main fmt println hello world func add a int b int int return a b footer",
    "created_at": "2024-10-31T05:51:09.807638",
    "updated_at": "2024-10-31T05:51:09.807641"
  },
  {
    "id": 17,
    "url": "https://kevinluzbetak.com/1-ai/PyTorch-Sentiment-Analysis-Model.html",
    "title": "PyTorch-Sentiment-Analysis-Model.html",
    "content": "sentiment analysis model build and save sentiment model usr bin env python import torch from torch utils data import dataset dataloader from torch import nn from transformers import berttokenizer bertmodel from sklearn metrics import accuracy_score classification_report define the textdataset class class textdataset dataset def __init__ self texts labels self texts texts self labels labels self tokenizer berttokenizer from_pretrained bert base uncased def __len__ self return len self texts def __getitem__ self idx text self texts idx label self labels idx tokens self tokenizer text padding max_length max_length 128 truncation true return_tensors pt input_ids tokens input_ids squeeze 0 attention_mask tokens attention_mask squeeze 0 return input_ids input_ids attention_mask attention_mask labels torch tensor label dtype torch float expanded dataset with both positive and negative examples text_data this movie is amazing positive i really disliked the plot negative the acting was superb positive this book is a masterpiece positive what a terrible waste of time negative i love this positive this was the worst movie ever negative fantastic experience loved it positive not good at all negative absolutely fantastic positive",
    "created_at": "2024-10-31T05:51:10.976394",
    "updated_at": "2024-10-31T05:51:10.976397"
  },
  {
    "id": 12,
    "url": "https://kevinluzbetak.com/1-ai/TF-IDF.html",
    "title": "TF-IDF.html",
    "content": "tf idf usr bin env python tf idf term frequency inverse document frequency from sklearn feature_extraction text import tfidfvectorizer from prettytable import prettytable example documents docs this is a sample document this document is another sample document machine learning document initialize the vectorizer vectorizer tfidfvectorizer fit the model and transform the text data into a tf idf matrix tfidf_matrix vectorizer fit_transform docs get feature names terms feature_names vectorizer get_feature_names_out convert to array to see the result tfidf_array tfidf_matrix toarray initialize prettytable table prettytable add column names terms table field_names document list feature_names add rows documents and their tf idf values rounded to 4 decimals for i doc in enumerate docs row f document i 1 round value 4 for value in tfidf_array i table add_row row print table document another document is learning machine sample this document 1 0 0 0 4091 0 5268 0 0 0 0 0 5268 0 5268 document 2 0 492 0 5812 0 3742 0 0 0 0 0 3742 0 3742 document 3 0 0 0 3854 0 0 0 6525 0 6525 0 0 0 0 tf idf value breakdown higher tf idf values the higher the tf idf score for a term in a document the more relevant or important that term is to that specific document lower tf idf values terms with lower tf idf scores are either less frequent or appear in many documents which reduces their importance in distinguishing between documents document 1 this is a sample document document another document is learning machine sample this document 1 0 0 0 4091 0 5268 0 0 0 0 0 5268 0 5268 this is sample these terms have a tf idf score of 0 5268 which indicates that they are equally important in this document they appear only once in the document and are relevant for its content document this word appears in both document 1 and document 2 making its score 0 4091 slightly lower because it s shared across documents reducing its uniqueness another learning machine these words are absent in this document so their tf idf score is 0 0 document 2 this document is another sample document document another document is learning machine sample this document 2 0 492 0 5812 0 3742 0 0 0 0 0 3742 0 3742 another this word has a high tf idf score of 0 492 because it only appears in document 2 making it more unique and relevant to this document document this word has the highest score of 0 5812 because it appears twice in document 2 making it more important than other terms is sample this these terms have lower scores 0 3742 because they also appear in document 1 so their importance in document 2 is reduced document 3 machine learning document document another document is learning machine sample this document 3 0 0 0 3854 0 0 0 6525 0 6525 0 0 0 0",
    "created_at": "2024-10-31T05:51:10.563933",
    "updated_at": "2024-10-31T05:51:10.563936"
  },
  {
    "id": 3,
    "url": "https://kevinluzbetak.com/1-ai/Stable-Diffusion-Web-UI.html",
    "title": "Stable-Diffusion-Web-UI.html",
    "content": "stable diffusion web ui stable diffusion web ui 1 clone the web ui repository stable diffusion webui git clone https github com automatic1111 stable diffusion webui git cd stable diffusion webui 2 install dependencies pip install r requirements txt if some dependencies fail try installing them manually pip install torch torchvision torchaudio index url https download pytorch org whl cpu 3 launch the web ui run the following command to start the web ui python launch py if you encounter the torch is not able to use gpu error use this command export commandline_args skip torch cuda test python launch py 4 access the web ui in your browser once the server is running the terminal will display a message like running on local url http 127 0 0 1 7860 open your web browser and go to http 127 0 0 1 7860 5 stopping the server to stop the web ui press ctrl c in the terminal where it s running optional run with mps for mac users if you re using a mac with an m1 m2 chip run with mps acceleration export pytorch_enable_mps_fallback 1 python launch py footer",
    "created_at": "2024-10-31T05:51:09.937118",
    "updated_at": "2024-10-31T05:51:09.937121"
  },
  {
    "id": 6,
    "url": "https://kevinluzbetak.com/1-ai/Recompile-FAISS-GPU-Installation.html",
    "title": "Recompile-FAISS-GPU-Installation.html",
    "content": "rebuild cuda gpu faiss gpu installation step 1 install dependencies run the following commands to install the necessary dependencies sudo apt get update sudo apt get install y cmake libopenblas dev libomp dev libgtest dev gcc 10 g 10 step 2 install cuda if cuda is not installed follow these steps to install it sudo apt get install cuda 12 2 set up environment variables for cuda export cuda_home usr local cuda 12 2 export path cuda_home bin path export ld_library_path cuda_home lib64 ld_library_path step 3 clone faiss repository git clone https github com facebookresearch faiss git cd faiss step 4 file editing modify cmakelists txt edit the cmakelists txt file to ensure proper configuration vi faiss cmakelists txt add the following lines at the top cmake_minimum_required version 3 27 project faiss version 1 7 0 languages c cxx fix the path for faiss config cmake in by editing the line around line 410 in cmakelists txt configure_file cmake_source_dir cmake faiss config cmake in cmake_binary_dir faiss config cmake only step 5 build faiss with gpu support cmake b build dfaiss_enable_gpu on dcmake_cuda_compiler usr local cuda 12 2 bin nvcc dcmake_cuda_architectures 86 dbuild_testing off dcmake_build_type release make j nproc step 6 install faiss optional sudo make install step 7 verify faiss installation run the following test script to verify faiss installation import faiss import numpy as np create random vectors d 128 dimension nb 1000 number of vectors xb np random random nb d",
    "created_at": "2024-10-31T05:51:10.082127",
    "updated_at": "2024-10-31T05:51:10.082129"
  },
  {
    "id": 2,
    "url": "https://kevinluzbetak.com/1-ai/Hugging-Face-Machine-Learning.html",
    "title": "Hugging-Face-Machine-Learning.html",
    "content": "hugging face machine learning hugging face login 1 login to hugging face cli if you haven t logged in yet authenticate via the command line huggingface cli login this will prompt you to enter your hugging face token you can generate a token from your hugging face account go to https huggingface co settings tokens create a new token with read access to the repository 2 verify repository access ensure you have the necessary permissions to access the restricted model visit the model page https huggingface co black forest labs flux 1 dev if the model is gated request access and wait for approval from the maintainers 3 set up api token if you re accessing the model programmatically configure the token properly export huggingface_token your_token_here or set it directly in your script from huggingface_hub import hfapi api hfapi api login token your_token_here 4 retry model download once authenticated run the command or script again to access the model note if you still encounter issues double check that the model owners have granted access if not you ll need to wait for their approval footer",
    "created_at": "2024-10-31T05:51:09.910420",
    "updated_at": "2024-10-31T05:51:09.910423"
  },
  {
    "id": 10,
    "url": "https://kevinluzbetak.com/1-ai/Time-Complexity-Big-O-Notation.html",
    "title": "Time-Complexity-Big-O-Notation.html",
    "content": "kevin luzbetak computer science big o notation time complexity big o notation is used to describe the efficiency of an algorithm focusing on its time complexity how the execution time grows with input size and space complexity how much extra memory is needed it expresses the worst case scenario performance of an algorithm common complexities o 1 constant time the algorithm s runtime does not change with the input size example accessing an element in an array by index o log n logarithmic time the algorithm reduces the problem size by a constant factor with each step example binary search in a sorted array o n linear time the runtime grows proportionally with the input size example traversing a list of n elements o n log n linearithmic time the algorithm performs a linear number of operations for each logarithmic division example merge sort and quicksort in their average cases o nÂ² quadratic time the runtime grows quadratically with input size example a double nested loop like in bubble sort or selection sort o 2 n exponential time the runtime doubles with each addition to the input size example solving the traveling salesman problem using brute force o n factorial time the runtime increases factorially with the input size example generating all permutations of a set data structures and their time complexities hash table access o 1 average case o n worst case due to collisions search o 1 average case insertion deletion o 1 average case b tree access search o log n insertion deletion o log n space complexity o n balanced binary search tree e g avl tree red black tree access search o log n insertion deletion o log n binary search tree bst access search o log n average o n worst case unbalanced insertion deletion o log n average o n worst case b tree balancing b trees are inherently balanced during indexing so they do not require rebalancing in the way that some other tree structures like avl trees or red black trees do balanced nature b trees are self balancing by design when you insert or delete keys the tree is adjusted to maintain its balance this is achieved by splitting or merging nodes as necessary during insertions and deletions ensuring that all leaf nodes remain at the same level and the tree remains balanced indexing during the indexing process as new keys are inserted into the b tree the structure automatically ensures that the tree remains balanced this is done by maintaining certain properties such as all nodes except the root having at least a minimum number of children the height of the tree being kept as low as possible to optimize search operations because of this b trees are particularly well suited for use in databases and file systems where efficient data retrieval and storage are critical array fixed size access o 1 search o n insertion o n if resizing required deletion o n linked list access o n search o n insertion deletion at head o 1 heap priority queue access max or min o 1 insertion deletion o log n graph adjacency matrix adjacency list search dfs bfs o v e where v is the number of vertices and e is the number of edges stack queue access o n insertion deletion o 1 footer",
    "created_at": "2024-10-31T05:51:10.394653",
    "updated_at": "2024-10-31T05:51:10.394656"
  },
  {
    "id": 9,
    "url": "https://kevinluzbetak.com/1-ai/Kevin_Luzbetak_Portfolio.html",
    "title": "Kevin_Luzbetak_Portfolio.html",
    "content": "kevin luzbetak professional portfolio kevin thomas luzbetak sr software and data engineer los angeles ca u s citizen kevinluzbetak gmail com portfolio phone 818 288 7357 software and data engineering summary kevin has over 25 years of hands on experience as a software and data engineer with expertise in devops orchestration and deployment pipelines he has worked extensively with databricks delta lake medallion architecture bronze silver gold snowflake and redshift focusing on building real time etl pipelines with apache kafka delta live tables dlt and other technologies to deliver high performance scalable and cost effective cloud solutions across aws azure and gcp kevin is proficient in data mesh architectures and decentralized data ownership management machine learning ai summary kevin develops predictive analytics solutions integrating machine learning models like llama and retrieval augmented generation rag systems to enhance the response generation process using external knowledge bases he is experienced with langchain and openai leveraging llms for chatbots content generation and real time ai tools additionally kevin uses streamlit to create interactive dashboards enabling seamless integration between data workflows and user interaction for machine learning models and predictive analytics solutions technical skills languages python pyspark sql golang scala java c bash machine learning pytorch scikit learn keras tensorflow nlp tableau pandas numpy cloud platforms aws s3 ec2 redshift lambda azure gcp data engineering databricks snowflake delta lake kafka airflow jenkins terraform docker devops monitoring kubernetes grafana datadog jenkins nagios professional experience sr software and data engineer disney streaming contract santa monica ca may 2023 march 2024 developed scalable products for lifecycle marketing analytics automating customer data science workflows utilized pyspark within databricks medallion architecture to build visualizations dashboards and full text search data dictionaries contributed to data migration from snowflake to databricks and participated in medallion architecture design sr software and data engineer nike inc contract beaverton or feb 2022 may 2023 designed and implemented pyspark data pipelines integrated with databricks delta lake medallion architecture developed an internal data catalog with full text search and tracking capabilities using python flask azure databricks data engineer brighthouse financial contract charlotte nc aug 2021 dec 2021 optimized data processing efficiency using azure devops and ambari cluster scaling collaborated with microsoft to resolve cluster issues and successfully migrated data from on premise to azure databricks senior data engineer apple inc contract cupertino ca jan 2021 april 2021 developed and maintained critical data pipelines for apple pay wallet and ci cd pipelines optimized workloads and managed infrastructure provisioning senior data engineer disqo inc glendale ca july 2020 nov 2020 implemented real time consumer insights predictive analytics using flask apis and aws services processed large datasets using emr athena and integrated third party apis google docs facebook redshift database engineer iii amazon web services east palo alto ca aug 2019 may 2020 led data migration from teradata and netezza to redshift ensuring performance and data integrity conducted proof of concept for clients to scale redshift clusters for handling massive datasets education master s degree in computer science california lutheran university 2012 2014 bachelor of science in information technology university of phoenix 2001 2004",
    "created_at": "2024-10-31T05:51:10.359777",
    "updated_at": "2024-10-31T05:51:10.359779"
  },
  {
    "id": 5,
    "url": "https://kevinluzbetak.com/1-ai/python-whoosh.html",
    "title": "python-whoosh.html",
    "content": "python whoosh create python bm25 index usr bin env python from whoosh index import create_in from whoosh fields import schema text id from whoosh import qparser from whoosh qparser import queryparser from whoosh analysis import stemminganalyzer import os define schema for indexing schema schema text stored true content text stored true analyzer stemminganalyzer stemming for better search results path id stored true unique true create index directory if it doesn t exist if not os path exists indexdir os mkdir indexdir create the index index create_in indexdir schema add documents to the index writer index writer example documents to index documents document 1 content the quick brown fox jumps over the lazy dog path a document 2 content whoosh is a fast search library implemented in pure python path b document 3 content the fox is quick and jumps high path c add each document to the index for doc in documents writer add_document doc content doc content path doc path writer commit save changes to the index print indexing completed search index bm25 usr bin env python from whoosh index import open_dir from whoosh qparser import queryparser def search query_str open the index ix open_dir indexdir parse the query with ix searcher as searcher query_parser queryparser content ix schema query query_parser parse query_str perform the search results searcher search query limit 10 print the results print f search results for query_str for result in results print f result path result path print f content result content print 40 example search queries search quick fox search search library output usr bin env python search results for quick fox document 3 path c content the fox is quick and jumps high document 1 path a content the quick brown fox jumps over the lazy dog search results for search library document 2 path b content whoosh is a fast search library implemented in pure python footer",
    "created_at": "2024-10-31T05:51:10.068033",
    "updated_at": "2024-10-31T05:51:10.068035"
  },
  {
    "id": 11,
    "url": "https://kevinluzbetak.com/1-ai/Tensors-Machine-Learning.html",
    "title": "Tensors-Machine-Learning.html",
    "content": "tensors multi dimensional array tensor in machine in essence a tensor is a multidimensional array used to represent data think of it as a generalization of vectors and matrices a scalar a single number is a 0 dimensional tensor a vector a list of numbers is a 1 dimensional tensor a matrix a table of numbers is a 2 dimensional tensor and we can have tensors with 3 4 or even more dimensions tensors are the fundamental data structure in deep learning frameworks like tensorflow and pytorch they allow us to efficiently represent and process complex data like images 3d tensors height width color channels videos 4d tensors time height width color channels and even natural language where words are embedded in high dimensional spaces simple python code with tensors using numpy numpy is a powerful library for numerical computations in python and provides excellent support for working with tensors which numpy calls ndarrays import numpy as np create a 2d tensor a matrix matrix np array 1 2 3 4 5 6 create a 3d tensor tensor_3d np array 1 2 3 4 5 6 7 8 access elements of the tensors print element at row 1 column 2 of the matrix matrix 1 2 print element at depth 0 row 1 column 0 of the 3d tensor tensor_3d 0 1 0 perform operations on tensors sum_of_matrix np sum matrix print sum of all elements in the matrix sum_of_matrix output element at row 1 column 2 of the matrix 6 element at depth 0 row 1 column 0 of the 3d tensor 3 sum of all elements in the matrix 21 in this code import the numpy library create a 2d tensor matrix and a 3d tensor tensor_3d using np array access specific elements using indexing remember indexing starts at 0 perform an operation summation on the matrix using np sum key points tensors enable us to represent and manipulate data with any number of dimensions deep learning frameworks heavily rely on tensors for efficient computation on gpus numpy provides a solid foundation for working with tensors in python import numpy as np create a 3d tensor with dimensions 2 3 4 tensor np zeros 2 3 4 fill the tensor with some values tensor 0 0 0 1 tensor 0 1 1 2 tensor 0 2 2 3 tensor 1 0 3 4 tensor 1 1 2 5 tensor 1 2 1 6 print the tensor print tensor access a specific value in the tensor e g the value at index 1 0 3 value tensor 1 0 3 print value sum all the values in the tensor total np sum tensor print total compute the mean of all the values in the tensor average np mean tensor print average in this example we create a 3d tensor with dimensions 2 3 4 using the np zeros function we then fill the tensor with some values using indexing we print the tensor access a specific value compute the sum and mean of all the values in the tensor using the np sum and np mean functions note that 3d tensors are also commonly used in deep learning frameworks such as tensorflow or pytorch the syntax for creating and manipulating 3d tensors in those frameworks is similar to numpy but with additional functionality for building and training machine learning models footer",
    "created_at": "2024-10-31T05:51:10.400621",
    "updated_at": "2024-10-31T05:51:10.400622"
  },
  {
    "id": 23,
    "url": "https://kevinluzbetak.com/1-ai/Vector-Database.html",
    "title": "Vector-Database.html",
    "content": "vector database vector database a vector database is a specialized type of database designed to efficiently store retrieve and query data in vector format vectors often representing numerical or feature embeddings from high dimensional data e g images text audio are used extensively in machine learning models these embeddings capture the essential characteristics of the data such as its semantic meaning by encoding it in vector space usage of vector databases in machine learning vector databases play a critical role in machine learning tasks where similarity search or clustering of high dimensional data is needed common usage scenarios recommendation systems retrieve similar items by finding nearest neighbors in vector space natural language processing nlp retrieve similar documents based on text embeddings image retrieval perform similarity search based on image embeddings anomaly detection identify abnormal behavior by clustering event vectors common machine learning techniques using vector databases nearest neighbor search k nn retrieve the nearest vectors for classification and regression tasks clustering k means dbscan store vectors for efficient clustering semantic search search for semantically similar text using text embeddings text embedding search store embeddings from models like bert or gpt to find similar documents image search store image embeddings for visual search applications recommendation systems recommend content based on similar user or item embeddings anomaly detection identify outliers in behavior or transaction data by detecting anomalies in vector space lancedb vector database overview lancedb is an open source vector database designed for efficient and fast storage retrieval and management of high dimensional vectors it focuses on providing real time performance and scalability for machine learning and ai applications lancedb is built for handling vector search workloads allowing users to store embeddings from text images or other data types and perform similarity searches with high efficiency key features open source and local hosting no api key required meaning it can be run locally or self hosted for full control optimized for vector search built specifically for storing vectors and performing nearest neighbor searches scalability lancedb can handle a wide range of workloads from small scale applications to large scale production environments integration with machine learning pipelines lancedb integrates well with ml pipelines making it ideal for ai driven applications such as recommendation systems semantic search and more real time search performance focuses on low latency queries and high throughput for fast real time vector searches use cases recommendation systems store embeddings of users or items and perform similarity searches to recommend products content or services semantic search use embeddings from nlp models like bert to find similar documents based on meaning rather than just keywords image search store image embeddings and retrieve similar images using vector similarity anomaly detection identify unusual data points by storing event vectors and detecting outliers using clustering and similarity searches why use lancedb performance oriented built to handle the performance needs of real time vector search applications machine learning friendly specifically designed to fit within the machine learning ecosystem making it easy to integrate with modern ai pipelines self hosted gives users full control over their data without the need for external apis or services python code using lancedb pip install lancedb import lancedb import numpy as np initialize the lancedb database db lancedb connect path to lancedb specify the path where the database will be stored create or connect to a collection similar to a table in a traditional db collection db create_collection example_collection generate random vector data e g 1000 vectors of dimensionality 128 vectors np random rand 1000 128 tolist insert vectors with associated metadata ids or labels data id i vector vec for i vec in enumerate vectors collection insert data query the collection for the nearest neighbors of a new vector query_vector np random rand 1 128 tolist 0 generate a random query vector results collection search query_vector limit 5 to_list limit the result to top 5 display the nearest neighbors and their distances for result in results print f id result id distance result distance key steps in the code connect to lancedb initializes a connection to lancedb at a specified path it can be local or a remote directory create a collection creates or connects to a collection which acts like a table for storing vectors and metadata insert data inserts 1000 randomly generated vectors into the collection each associated with an id query for nearest neighbors uses a randomly generated query vector to search for the top 5 most similar vectors in the collection explanation vectors in this example 1000 random vectors of dimensionality 128 are generated in real world applications these vectors could represent embeddings from text images or other high dimensional data search the search function performs a nearest neighbor search to find the most similar vectors in the collection based on distance e g euclidean or cosine similarity faiss facebook ai similarity search faiss is an open source library developed by facebook ai for efficient similarity search and clustering of dense vectors it is highly optimized for handling very large datasets of high dimensional vectors key features no api key required since it s a local library scales to billions of vectors offers various index types such as flat hnsw and ivf inverted file index supports both gpu and cpu for faster performance use case faiss can be used in scenarios like image search recommendation systems and nlp embedding retrieval pip install faiss cpu or faiss gpu for gpu version import faiss import numpy as np create an index for l2 distance euclidean d 128 dimension of vectors index faiss indexflatl2",
    "created_at": "2024-10-31T05:51:11.028366",
    "updated_at": "2024-10-31T05:51:11.028368"
  },
  {
    "id": 21,
    "url": "https://kevinluzbetak.com/1-ai/Keras.html",
    "title": "Keras.html",
    "content": "keras training neural networks keras overview keras is an open source deep learning library that provides a high level api for building and training neural networks it is user friendly modular and extensible allowing developers to create complex models with minimal code keras runs on top of low level deep learning frameworks such as tensorflow theano and cntk making it both versatile and powerful key features ease of use keras offers a simple and consistent api to build neural networks quickly modular it allows you to build models by combining different building blocks layers optimizers loss functions flexibility keras can be used for a variety of tasks including image classification text generation reinforcement learning and more support for both cpu and gpu keras can run on both cpu and gpu hardware enabling faster training when using gpus backed by tensorflow keras is now part of the tensorflow core library making it the default high level api for tensorflow 1 feedforward neural network classification task a feedforward neural network fnn is a type of artificial neural network where the connections between nodes do not form a cycle it is called feedforward because the data flows only in one direction from the input layer to the output layer without any loops or feedback connections structure input layer this layer receives the input features each node in the input layer corresponds to one feature of the data hidden layers these are the layers where most of the computation occurs the hidden layers use activation functions like relu rectified linear unit to introduce non linearity allowing the network to learn complex patterns in the data output layer this layer produces the final prediction for classification tasks the output layer typically uses a sigmoid binary classification or softmax multi class classification activation function to output probabilities for each class example of a classification task in the example of classifying data with synthetic features the network learns to map the input features e g 20 features to the correct class labels 0 or 1 by adjusting the weights and biases during training the process involves forward propagation calculating output and backpropagation adjusting weights based on the loss function and optimizer like adam it is suitable for basic tasks like classifying tabular data use case binary or multi class classification on structured data such as customer churn prediction medical diagnosis or fraud detection import numpy as np from tensorflow keras models import sequential from tensorflow keras layers import dense from sklearn datasets import make_classification from sklearn model_selection import train_test_split from sklearn preprocessing import standardscaler generate synthetic data for classification x y make_classification n_samples 1000 n_features 20 n_classes 2 random_state 42 split the data into training and testing sets x_train x_test y_train y_test train_test_split x y test_size 0 2 random_state 42 normalize the features scaler standardscaler x_train scaler fit_transform x_train x_test scaler transform x_test define the model model sequential model add dense 32 input_dim 20 activation relu input layer model add dense 16 activation relu hidden layer model add dense 1 activation sigmoid output layer for binary classification compile the model model compile optimizer adam loss binary_crossentropy metrics accuracy train the model model fit x_train y_train epochs 10 batch_size 32 validation_data x_test y_test evaluate the model loss accuracy model evaluate x_test y_test print f accuracy accuracy 2f 2 convolutional neural network cnn for image classification a convolutional neural network cnn is a specialized neural network primarily used for processing structured grid like data such as images cnns are particularly effective at recognizing spatial hierarchies in data making them ideal for tasks like image and video recognition structure input layer the input to a cnn is typically a multi dimensional image e g a grayscale image with a size of 28x28 pixels has one channel while a colored image has 3 channels rgb convolutional layers these layers apply convolution operations to the input using filters kernels to detect features like edges corners and textures the output of each convolutional layer is a feature map that highlights these patterns pooling layers these layers reduce the spatial dimensions of the feature maps e g max pooling helping to retain important information while reducing computational complexity flattening after the convolutional and pooling layers the 2d feature maps are flattened into a 1d vector that can be fed into a fully connected layer fully connected layers these layers are similar to a feedforward network and are used to make the final classification decision the output layer typically uses a softmax activation function for multi class classification example of image classification in the mnist digit classification example the network takes a 28x28 pixel grayscale image as input passes it through convolutional layers to detect edges and patterns and eventually predicts the digit class 0 9 using fully connected layers cnns are excellent at reducing the number of parameters by reusing the filters across the image making them highly efficient for image processing tasks use case image classification tasks such as handwriting recognition object detection medical image analysis and facial recognition from tensorflow keras models import sequential from tensorflow keras layers import conv2d maxpooling2d flatten dense from tensorflow keras datasets import mnist from tensorflow keras utils import to_categorical load the mnist dataset handwritten digits x_train y_train x_test y_test mnist load_data reshape the data to fit the model x_train x_train reshape 1 28 28 1 x_test x_test reshape 1 28 28 1 normalize the data x_train x_train 255 0 x_test x_test 255 0 one hot encode the labels y_train to_categorical y_train 10 y_test to_categorical y_test 10 define the cnn model model sequential model add conv2d 32 kernel_size 3 3 activation relu input_shape 28 28 1 model add maxpooling2d pool_size 2 2 model add conv2d 64 kernel_size 3 3 activation relu model add maxpooling2d pool_size 2 2 model add flatten model add dense 128 activation relu model add dense 10 activation softmax output layer for 10 classes compile the model model compile optimizer adam loss categorical_crossentropy metrics accuracy train the model model fit x_train y_train epochs 5 batch_size 64 validation_data x_test y_test evaluate the model loss accuracy model evaluate x_test y_test print f test accuracy accuracy 2f summary of differences feedforward neural network best for structured tabular data where features are not spatially related convolutional neural network best for image or grid like data where spatial hierarchies matter e g recognizing objects or patterns in images key components in keras models the core structure is either sequential a linear stack of layers or functional api more flexible for complex models layers building blocks of neural networks such as dense conv2d lstm etc optimizers algorithms for adjusting weights during training e g adam sgd loss functions used to minimize the error in predictions e g binary_crossentropy mean_squared_error metrics metrics like accuracy used to evaluate model performance keras provides a highly accessible platform for building deep learning models quickly while abstracting many low level complexities footer",
    "created_at": "2024-10-31T05:51:11.010738",
    "updated_at": "2024-10-31T05:51:11.010742"
  },
  {
    "id": 20,
    "url": "https://kevinluzbetak.com/1-ai/Random-Forest-Classifier-Model.html",
    "title": "Random-Forest-Classifier-Model.html",
    "content": "random forest classifier model random forest classifieri with tf idf vectorizer this model consists of a collection of decision trees the forest where each tree is trained on a random subset of the data the final prediction is made by averaging the predictions of all the individual trees which helps reduce overfitting and improves generalization input features the text data customer questions is vectorized using the tf idf vectorizer tfidfvectorizer to convert the textual information into numerical features that the model can understand task the model is trained to classify customer support questions into various categories such as product support billing order etc given a new customer query the model can predict which category it belongs to usr bin env python import pandas as pd from sklearn model_selection import train_test_split from sklearn feature_extraction text import tfidfvectorizer from sklearn ensemble import randomforestclassifier from sklearn metrics import accuracy_score from prettytable import prettytable step 1 create data model dictionary how do i reset my password product support what are the payment options billing where can i find my purchase history billing how can i contact support product support can i get a refund billing why was i charged twice billing how do i change my subscription product support what is the return policy billing how do i track my order product support how do i update my billing information billing how do i troubleshoot my device product support why is my payment not going through billing how can i cancel my order product support what should i do if i receive a damaged item product support where can i see my account balance billing how do i apply a discount code billing can i exchange a product i purchased product support how do i upgrade my plan product support why was my account suspended product support can i change the shipping address for my order product support where do i find the user manual product support how do i report a missing item in my order product support how do i enable two factor authentication product support what is the warranty on my product product support how do i pay my outstanding balance billing how can i verify my payment method billing why was my refund declined billing can i pay using cryptocurrency billing how do i dispute a charge billing where do i see my payment history billing what do i do if i forgot my login credentials product support how do i reset my account pin product support what happens if my subscription expires product support where can i view my invoice billing how do i access technical support product support what are the supported payment methods billing can i change my delivery date product support how do i redeem a gift card billing how do i unsubscribe from email notifications billing what do i do if my card is declined billing can i update my order before it ships product support how can i find the nearest store billing why is my bill higher than expected billing what should i do if i forgot my password product support how do i cancel my subscription product support how do i track my refund billing why was my payment declined billing how do i contact technical support product support how do i change my account details product support ensure the lists have the same length df pd dataframe list dictionary items columns question category step 2 train test split x_train x_test y_train y_test train_test_split df question df category test_size 0 3 random_state 42 step 3 vectorize text data vectorizer tfidfvectorizer x_train_vec vectorizer fit_transform x_train",
    "created_at": "2024-10-31T05:51:10.996967",
    "updated_at": "2024-10-31T05:51:10.996969"
  },
  {
    "id": 18,
    "url": "https://kevinluzbetak.com/1-ai/Scikit-learn.html",
    "title": "Scikit-learn.html",
    "content": "scikit learn scikit learn overview scikit learn is a widely used open source python library for machine learning providing simple and efficient tools for data analysis and modeling it is built on top of popular libraries like numpy scipy and matplotlib and offers a wide range of algorithms for supervised and unsupervised learning key features preprocessing tools for scaling normalization encoding categorical variables and handling missing data model selection cross validation grid search and hyperparameter tuning for selecting the best models supervised learning algorithms like linear regression support vector machines svm decision trees and random forests unsupervised learning algorithms for clustering k means dbscan and dimensionality reduction pca t sne metrics a variety of metrics for evaluating model performance such as accuracy precision recall and auc roc model persistence allows saving trained models for later use via joblib or pickle scikit learn is highly accessible for both beginners and professionals with well documented apis and good integration with other data science tools like pandas from sklearn model_selection import train_test_split from sklearn datasets import load_iris from sklearn ensemble import randomforestclassifier from sklearn metrics import accuracy_score load the iris dataset data load_iris x data data features y data target target labels split the dataset into training and testing sets 80 train 20 test x_train x_test y_train y_test train_test_split x y test_size 0 2 random_state 42 initialize a random forest classifier clf randomforestclassifier n_estimators 100 random_state 42 train the classifier clf fit x_train y_train make predictions on the test set y_pred clf predict x_test calculate the accuracy of the model accuracy accuracy_score y_test y_pred print the accuracy print f accuracy accuracy 2f print a comparison of actual vs predicted values print nactual vs predicted for actual predicted in zip y_test y_pred print f actual actual predicted predicted print some sample data points from the test set to explain the results print nsample data points features from the test set for i in range 5 print f test sample",
    "created_at": "2024-10-31T05:51:10.984624",
    "updated_at": "2024-10-31T05:51:10.984627"
  },
  {
    "id": 13,
    "url": "https://kevinluzbetak.com/1-ai/bm25-probabilistic-information-retrieval-model.html",
    "title": "bm25-probabilistic-information-retrieval-model.html",
    "content": "bm25 probabilistic information retrieval model bm25 best matching 25 is a ranking function used by search engines to rank documents based on their relevance to a given query it is one of the most well known algorithms within the family of probabilistic information retrieval models bm25 builds upon the earlier tf idf term frequency inverse document frequency approach and is considered a highly effective ranking model for information retrieval tasks key components of bm25 term frequency tf measures how often a term keyword appears in a document bm25 uses a variant called saturated term frequency meaning that the importance of term frequency grows less rapidly as it increases this prevents overemphasis on documents with high repetition of a term inverse document frequency idf measures how unique or rare a term is across the entire document collection corpus terms that appear in many documents are given less weight while terms that appear in fewer documents are considered more important document length normalization bm25 takes into account the length of a document to avoid favoring longer documents that may mention terms more frequently simply because they contain more content the algorithm normalizes the term frequency based on document length adjusting the importance of terms in shorter or longer documents bm25 formula the bm25 relevance score for a document d and a query q is calculated as bm25 d q t q idf t f t d k1 1 f t d",
    "created_at": "2024-10-31T05:51:10.763045",
    "updated_at": "2024-10-31T05:51:10.763048"
  },
  {
    "id": 7,
    "url": "https://kevinluzbetak.com/1-ai/Streamlit-app.html",
    "title": "Streamlit-app.html",
    "content": "streamlit app streamlit app enter your name select your age 25 hello kevin you are 25 years old line chart a b c 0 17 0 57 1 22 0 47 1 11 0 34 0 33 0 88 0 45 1 12 1 34 0 76 1 02 0 66 0 99 usr bin env python import streamlit as st import pandas as pd import numpy as np of the app st streamlit app text input box user_name st text_input enter your name kevin slider input age st slider select your age 1 100 25 display user input st write f",
    "created_at": "2024-10-31T05:51:10.318412",
    "updated_at": "2024-10-31T05:51:10.318414"
  },
  {
    "id": 14,
    "url": "https://kevinluzbetak.com/1-ai/Retrieval-Augmented-Generation.html",
    "title": "Retrieval-Augmented-Generation.html",
    "content": "rag retrieval augmented generation building a high performance rag solution with pgvectorscale and python 1 rag retrieval augmented generation rag enhances the response generation process by retrieving relevant documents from an external knowledge base e g a vector database and using these documents to inform the generated responses it combines retriever finds relevant documents based on a query generator uses the retrieved documents to generate a more accurate and informed response 2 pgvectorscale pgvectorscale is an extension for postgresql that enables high performance vector similarity searches using embeddings it builds on pgvector allowing storage and indexing of high dimensional vectors making it suitable for large scale rag systems 3 setting up the components to build the rag solution you ll need a vector database postgresql with pgvector pgvectorscale to store embeddings embeddings for your documents generated via models like gpt bert etc a retrieval algorithm to find relevant documents a generation model such as gpt to use the retrieved documents for response generation 4 build a high performance rag solution step 1 install postgresql pgvector and pgvectorscale install the necessary components create extension vector create extension pgvectorscale step 2 generate embeddings use a model to create embeddings for your documents from sentence_transformers import sentencetransformer model sentencetransformer all minilm l6 v2 documents your document text here another document text embeddings model encode documents step 3 store embeddings in postgresql create a table to store the embeddings create table documents id serial primary key text text embedding vector 768 insert documents and embeddings import psycopg2 conn psycopg2 connect dbname test user postgres cur conn cursor cur execute insert into documents text embedding values s s document_text embedding tolist conn commit step 4 retrieve relevant documents query the vector database for the most relevant documents select from documents order by embedding query_embedding limit 5 step 5 generate responses using the retrieved documents use a generation model like gpt to create a response from transformers import gpt2lmheadmodel gpt2tokenizer model gpt2lmheadmodel from_pretrained gpt2 tokenizer gpt2tokenizer from_pretrained gpt2 prompt f based on the following documents n retrieved_documents nanswer the question user_query inputs tokenizer prompt return_tensors pt outputs model generate inputs input_ids max_length 500 response tokenizer decode outputs 0 print response 5 optimization for high performance to ensure high performance use indexing and parallel queries in postgresql for large scale datasets distribute the retrieval tasks across multiple nodes 6 use cases customer support augment chatbot answers with external documents search engines provide contextual responses to user queries knowledge management retrieve and synthesize information from large repositories references high performance rag solution with pgvectorscale and python tutorial build high performance rag footer",
    "created_at": "2024-10-31T05:51:10.810345",
    "updated_at": "2024-10-31T05:51:10.810348"
  },
  {
    "id": 15,
    "url": "https://kevinluzbetak.com/1-ai/PyTorch.html",
    "title": "PyTorch.html",
    "content": "pytorch neural network pytorch machine learning library pytorch is an open source machine learning library used for a wide variety of tasks such as deep learning natural language processing nlp and computer vision it provides a flexible platform to build machine learning models and comes with strong support for gpu acceleration making it popular among researchers and developers key features of pytorch tensors pytorch provides multi dimensional arrays similar to numpy but with gpu support for faster computation autograd pytorch uses automatic differentiation to compute gradients making it easier to implement and optimize neural networks dynamic computational graphs pytorch builds computational graphs dynamically making it easier to debug and modify deep learning models pytorch includes pre built models in the torchvision and torchtext libraries for various computer vision and nlp tasks extensibility you can build custom layers models and optimization techniques pytorch example how to use pytorch to create a neural network for classifying mnist digits usr bin env python import torch import torch nn as nn import torch optim as optim import torchvision import torchvision transforms as transforms define a simple neural network class net nn module def __init__ self super net self __init__ self fc1 nn linear 28 28 128 fully connected layer 1 self fc2 nn linear 128 10 fully connected layer 2 10 classes def forward self x x x view 1 28 28 flatten the image x torch relu self fc1 x apply relu to fc1 x self fc2 x final output logits return x load dataset and preprocess transform transforms compose transforms totensor transforms normalize 0 5 0 5 trainset torchvision datasets mnist root data train true download true transform transform trainloader torch utils data dataloader trainset batch_size 64 shuffle true initialize the model loss function and optimizer net net criterion nn crossentropyloss optimizer optim sgd net parameters lr 0 01 training loop for epoch in range 2 running_loss 0 0 for inputs labels in trainloader optimizer zero_grad zero the gradients outputs net inputs forward pass loss criterion outputs labels compute the loss loss backward backward pass optimizer step optimize running_loss loss item print f epoch epoch 1 loss running_loss len trainloader print finished training common pytorch libraries torchvision contains datasets models and transforms for computer vision torchtext for nlp models and datasets torchaudio for audio and speech processing footer",
    "created_at": "2024-10-31T05:51:10.812362",
    "updated_at": "2024-10-31T05:51:10.812363"
  },
  {
    "id": 19,
    "url": "https://kevinluzbetak.com/1-ai/faiss/Indexing-FAISS-OpenAI-Embeddings.html",
    "title": "Indexing-FAISS-OpenAI-Embeddings.html",
    "content": "email processing with faiss and openai embeddings email processing with faiss and openai embeddings description this python script performs the following tasks loads email data from a json file full_emails json extracts relevant fields such as content id and timestamp using a schema cleans the email content by removing urls extra spaces and unnecessary text converts the cleaned content into document objects for further processing generates embeddings for the email documents using openaiembeddings stores the embeddings in a faiss vector database for fast similarity search saves the faiss index to disk for future use usr bin env python import json import re import os from datetime import datetime from langchain_community document_loaders import jsonloader from langchain_openai import openaiembeddings updated import from langchain_community vectorstores import faiss from langchain schema import document import document class define jq schema to extract relevant fields from the json jq_schema page_content content metadata id i d date date initialize the jsonloader with the schema loader jsonloader full_emails json jq_schema jq_schema text_content false documents loader load initialize openai embeddings embeddings openaiembeddings clean email content function def clean_content text text re sub r http s text remove urls text re sub r 2 text replace sequences of dashes with one dash text re sub r unsubscribe learn why we included you are receiving text flags re ignorecase text re sub r image w text remove entities return re sub r s text strip remove extra spaces convert the cleaned content into document objects processed_docs for doc in documents try content_data json loads doc page_content parse content content clean_content content_data get page_content no content metadata content_data get metadata email_id metadata get id unknown id timestamp int metadata get date 0 1000 convert to seconds date datetime fromtimestamp timestamp isoformat if timestamp 0 else",
    "created_at": "2024-10-31T05:51:10.990810",
    "updated_at": "2024-10-31T05:51:10.990813"
  },
  {
    "id": 22,
    "url": "https://kevinluzbetak.com/1-ai/faiss/RetrievalQA-FAISS-with-OpenAI-GPT-4.html",
    "title": "RetrievalQA-FAISS-with-OpenAI-GPT-4.html",
    "content": "faiss retrieval qa with openai gpt 4 retrievalqa retrievalqa is a specialized question answering framework that leverages retrievers like faiss or vector databases to find relevant documents or text snippets based on a query and then uses language models llms to generate answers based on the retrieved content this architecture improves the llm s ability to answer questions accurately by limiting the scope to relevant data instead of relying on the model s general knowledge in this framework retriever quickly fetches relevant documents or information from a knowledge base e g a faiss index language model llm uses retrieved content as context to generate precise answers to user questions faiss retrieval qa with openai gpt 4 description this python script builds a conversational qa system by integrating faiss vector stores with openai gpt 4 it retrieves relevant documents from the faiss index to answer user questions interactively below is a step by step breakdown of the script s functionality script breakdown api key setup the openai api key is retrieved from an environment variable openai_api_key if the key is not set the script raises an error loading faiss index the faiss vector index is loaded from disk allowing similarity search across indexed content if the index is missing the script raises a filenotfounderror initializing gpt 4 uses the chatopenai model to provide answers based on the retrieved documents building the qa chain a retrievalqa chain is constructed to connect the faiss retriever and the gpt 4 model for end to end question answering query loop the user can interact with the system in a loop asking questions if relevant documents are found the answer is displayed if no documents match the user is notified python code usr bin env python import os import openai ensure openai is imported from langchain_openai import openaiembeddings chatopenai updated imports from langchain_community vectorstores import faiss from langchain chains import retrievalqa step 1 set up api key openai api_key os getenv openai_api_key if not openai api_key raise valueerror openai api key not found set openai_api_key as an environment variable step 2 load the faiss index from disk faiss_index_path faiss_index embeddings openaiembeddings if os path exists faiss_index_path print f loading faiss index from faiss_index_path vector_store faiss load_local faiss_index_path embeddings allow_dangerous_deserialization true enable safe deserialization else raise filenotfounderror f",
    "created_at": "2024-10-31T05:51:11.018957",
    "updated_at": "2024-10-31T05:51:11.018960"
  },
  {
    "id": 16,
    "url": "https://kevinluzbetak.com/1-ai/faiss/Gmail-Email-Fetch.html",
    "title": "Gmail-Email-Fetch.html",
    "content": "gmail email fetch gmail email fetch script description this python script uses the gmail api to authenticate a user and retrieve the last 1000 emails from their gmail account the email data is saved as a json file on the local system below is a breakdown of the process and the complete script script breakdown oauth2 authentication uses google oauth credentials to authenticate the user if valid credentials are found in token json they are reused otherwise a new login prompt is initiated fetching emails after authentication the script retrieves up to 1000 emails and extracts key details such as email id snippet of content internal date timestamp payload metadata saving emails all fetched email data is stored in last_1000_emails json with readable formatting usr bin env python import os import json from google oauth2 credentials import credentials from google_auth_oauthlib flow import installedappflow from googleapiclient discovery import build define the oauth scope for read only gmail access scopes https www googleapis com auth gmail readonly authenticate and create gmail api service def authenticate_gmail creds none if os path exists token json creds credentials from_authorized_user_file token json scopes if there are no valid credentials authenticate the user if not creds or not creds valid flow installedappflow from_client_secrets_file credentials json scopes creds flow run_local_server port 0 save credentials for future runs with open token json w as token_file token_file write creds to_json return build gmail v1 credentials creds fetch the last 1000 emails and save them to a json file def fetch_and_dump_emails service authenticate_gmail results service users messages list userid me maxresults 1000 execute messages results get messages emails for msg in messages msg_detail service users messages get userid me i",
    "created_at": "2024-10-31T05:51:10.874252",
    "updated_at": "2024-10-31T05:51:10.874255"
  }
]